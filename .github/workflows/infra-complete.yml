name: Complete Infrastructure Deploy

on:
  workflow_dispatch:
  push:
    branches: [ main, groupTest ]
    paths:
      - 'k8s/**'
      - 'terraform/**'
      - '.github/workflows/infra-complete.yml'

jobs:
  terraform_apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: terraform
    env:
      TF_VAR_sql_password: ${{ secrets.SQL_ADMIN_PASSWORD }}
      TF_VAR_location: ${{ secrets.AZURE_LOCATION || vars.AZURE_LOCATION || 'austriaeast' }}
    outputs:
      kube_config_data: ${{ steps.export_kubeconfig.outputs.kube_config_data }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.13.3
          terraform_wrapper: false

      - name: Show Terraform version and path
        id: tfver
        run: |
          terraform version
          echo "TF_BIN=$(command -v terraform)" >> "$GITHUB_ENV"

      - name: Terraform Init (with backend)
        run: $TF_BIN init -input=false -upgrade

      - name: Terraform Apply
        run: $TF_BIN apply -auto-approve -lock=false -input=false

      # Export AKS kubeconfig dynamically
      - name: Export AKS kubeconfig
        id: export_kubeconfig
        shell: bash
        run: |
          echo "Fetching AKS credentials dynamically..."
          az aks get-credentials \
            --resource-group rg-group1-austriaeast \
            --name grp1-aks \
            --file kubeconfig \
            --overwrite-existing

          echo "Encoding kubeconfig to base64..."
          KUBE64=$(base64 -w0 kubeconfig)
          echo "::set-output name=kube_config_data::$KUBE64"
          echo "‚úÖ Kubeconfig exported successfully."

  deploy_ingress:
    name: Deploy Ingress NGINX
    runs-on: ubuntu-latest
    needs: terraform_apply
    env:
      KUBE_CONFIG_DATA: ${{ needs.terraform_apply.outputs.kube_config_data }}
    outputs:
      lb_ip: ${{ steps.lb_ip.outputs.ip }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Set AKS Context
        uses: azure/aks-set-context@v3
        with:
          resource-group: rg-group1-austriaeast
          cluster-name: grp1-aks
      
      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version
      

      - name: Install or Upgrade Ingress NGINX (Safe)
        run: |
         set -e
         echo "üì¶ Adding ingress-nginx Helm repo..."
         helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
         helm repo update

         echo "üöÄ Deploying Ingress NGINX (install or upgrade)..."
         helm upgrade --install grp1-ingress ingress-nginx/ingress-nginx \
           --namespace ingress-nginx \
           --create-namespace \
           --wait \
           --timeout=10m \
           --atomic \
           --set controller.ingressClassByName=true \
           --set controller.ingressClassResource.controllerValue="k8s.io/ingress-nginx" \
           --set controller.service.type=LoadBalancer \
           --set controller.service.externalTrafficPolicy=Local || echo "‚ö†Ô∏è Helm install may have timed out, continuing..."

         echo "üîç Checking ingress-nginx pods status..."
         kubectl get pods -n ingress-nginx || true

         echo "üîç Checking ingress-nginx services..."
         kubectl get svc -n ingress-nginx || true

         echo "üîç Checking any pending pods..."
         kubectl get pods -A | grep Pending || true

         echo "‚úÖ Ingress NGINX deployment step finished (even if Helm wait timed out)"


      
      - name: Wait for LoadBalancer IP
        id: lb_ip
        run: |
          echo "üîç Checking for existing LoadBalancer IP..."
          
          # Check if LoadBalancer already has an IP
          LB_IP=$(kubectl get svc grp1-ingress-ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
          
          if [ -n "$LB_IP" ] && [ "$LB_IP" != "null" ] && [ "$LB_IP" != "None" ] && [ "$LB_IP" != "" ]; then
            echo "‚úÖ LoadBalancer IP already exists: $LB_IP"
            echo "ip=$LB_IP" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "‚è≥ Waiting for new LoadBalancer IP..."
          timeout=600  # 10 minutes timeout
          elapsed=0
          while [ $elapsed -lt $timeout ]; do
            LB_IP=$(kubectl get svc grp1-ingress-ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
            if [ -n "$LB_IP" ] && [ "$LB_IP" != "null" ] && [ "$LB_IP" != "None" ] && [ "$LB_IP" != "" ]; then
              echo "‚úÖ LoadBalancer IP obtained: $LB_IP"
              echo "ip=$LB_IP" >> $GITHUB_OUTPUT
              exit 0
            fi
            echo "‚è≥ Waiting for IP... ($elapsed seconds)"
            sleep 15
            elapsed=$((elapsed + 15))
          done
          echo "‚ö†Ô∏è Timeout waiting for LoadBalancer IP after $timeout seconds"
          echo "Checking service status..."
          kubectl get svc grp1-ingress-ingress-nginx-controller -n ingress-nginx -o wide
          exit 1
      
      - name: Display LoadBalancer Info
        run: |
          kubectl get svc grp1-ingress-ingress-nginx-controller -n ingress-nginx
          echo "‚úÖ Ingress NGINX installed with IP: ${{ steps.lb_ip.outputs.ip }}"

  k8s_apply:
    name: Kubernetes Apply
    runs-on: ubuntu-latest
    needs: [terraform_apply, deploy_ingress]
    env:
      KUBE_CONFIG_DATA: ${{ needs.terraform_apply.outputs.kube_config_data }}
      LB_IP: ${{ needs.deploy_ingress.outputs.lb_ip }}
      K8S_NAMESPACE: app
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Detect k8s manifests
        id: detect
        run: |
          set -euo pipefail
          mkdir -p k8s || true
          
          # Find only valid Kubernetes manifest files
          files=$(find k8s -type f \( -name '*.yml' -o -name '*.yaml' \) -size +0c | while read -r file; do
            if grep -q "apiVersion:" "$file" && grep -q "kind:" "$file"; then
              echo "$file"
            fi
          done | sort || true)
          
          if [ -n "$files" ]; then
            echo "found=true" >> "$GITHUB_OUTPUT"
            echo "Valid Kubernetes manifests found:"
            cat <<< "$files"
          else
            echo "found=false" >> "$GITHUB_OUTPUT"
            echo "‚ùå No valid Kubernetes manifests found, skipping."
          fi

      - name: Setup kubectl
        if: steps.detect.outputs.found == 'true'
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.30.0'

      # Prepare kubeconfig from dynamic Terraform output
      - name: Prepare kubeconfig
        if: steps.detect.outputs.found == 'true'
        run: |
          mkdir -p ~/.kube
          echo "${KUBE_CONFIG_DATA}" | base64 -d > ~/.kube/config
          chmod 600 ~/.kube/config
          echo "‚úÖ Kubeconfig loaded successfully"
          kubectl cluster-info

      - name: Ensure app namespace exists
        if: steps.detect.outputs.found == 'true'
        run: |
          if ! kubectl get namespace app > /dev/null 2>&1; then
            echo "Creating namespace 'app'..."
            kubectl create namespace app
          else
            echo "‚úÖ Namespace 'app' already exists"
          fi

      - name: Update Frontend ConfigMap with LB IP
        if: steps.detect.outputs.found == 'true'
        run: |
          if [ -z "$LB_IP" ] || [ "$LB_IP" == "null" ]; then
            echo "‚ùå LoadBalancer IP is not available"
            exit 1
          fi
          
          echo "Updating ConfigMap with LoadBalancer IP: $LB_IP"
          kubectl create configmap frontend-config -n app \
            --from-literal=VITE_API_BASE_URL="http://$LB_IP.nip.io/app-api" \
            --dry-run=client -o yaml | kubectl apply -f -
          
          echo "‚úÖ Frontend ConfigMap updated with IP: $LB_IP"

      - name: Apply/Update Kubernetes manifests
        if: steps.detect.outputs.found == 'true'
        run: |
          set -euo pipefail
          echo "üîÑ Applying/Updating manifests to cluster..."

          if [ -f k8s/namespace.yaml ]; then
            kubectl apply -f k8s/namespace.yaml
            echo "‚úÖ Namespace applied/updated"
          elif [ -f k8s/namespace.yml ]; then
            kubectl apply -f k8s/namespace.yml
            echo "‚úÖ Namespace applied/updated"
          fi

          # Apply manifests excluding non-Kubernetes files
          echo "üîÑ Applying/Updating Kubernetes manifests..."
          find k8s -name "*.yaml" -o -name "*.yml" | while read -r file; do
            # Skip files that are not Kubernetes manifests
            if grep -q "apiVersion:" "$file" && grep -q "kind:" "$file"; then
              echo "üîÑ Applying/Updating $file..."
              kubectl apply -f "$file" || echo "‚ö†Ô∏è Failed to apply $file, continuing..."
            else
              echo "‚è≠Ô∏è Skipping $file (not a Kubernetes manifest)"
            fi
          done
          
          echo "‚úÖ All manifests applied/updated successfully."

      - name: Verify Deployment Rollouts
        if: steps.detect.outputs.found == 'true'
        continue-on-error: true
        run: |
          kubectl -n "$K8S_NAMESPACE" get deploy || exit 0
          kubectl -n "$K8S_NAMESPACE" rollout status deploy --timeout=120s

  deploy_monitoring:
    name: Deploy Monitoring Stack
    runs-on: ubuntu-latest
    needs: [terraform_apply, deploy_ingress]
    env:
      KUBE_CONFIG_DATA: ${{ needs.terraform_apply.outputs.kube_config_data }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Set AKS Context
        uses: azure/aks-set-context@v3
        with:
          resource-group: rg-group1-austriaeast
          cluster-name: grp1-aks
      
      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version
      
      - name: Deploy/Upgrade Monitoring Stack
        run: |
          echo "üöÄ Deploying/Upgrading Prometheus monitoring stack..."
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          
          # Check if monitoring is already installed
          if helm list -n monitoring | grep -q kube-prometheus-stack; then
            echo "üìä Monitoring stack already installed, upgrading..."
            helm upgrade kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              --namespace monitoring \
              --values k8s/monitoring/values.yaml \
              --wait \
              --timeout=10m
            echo "‚úÖ Monitoring stack upgraded successfully"
          else
            echo "üìä Installing monitoring stack..."
            helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              --namespace monitoring \
              --create-namespace \
              --values k8s/monitoring/values.yaml \
              --wait \
              --timeout=10m
            echo "‚úÖ Monitoring stack deployed successfully"
          fi
      
      - name: Apply Monitoring Manifests
        run: |
          echo "üìã Applying ServiceMonitors and PrometheusRules..."
          
          # Apply monitoring manifests if they exist
          if [ -f k8s/monitoring/backend-monitor.yaml ]; then
            kubectl apply -f k8s/monitoring/backend-monitor.yaml
            echo "‚úÖ Backend ServiceMonitor applied"
          fi
          
          if [ -f k8s/monitoring/frontend-monitor.yaml ]; then
            kubectl apply -f k8s/monitoring/frontend-monitor.yaml
            echo "‚úÖ Frontend ServiceMonitor applied"
          fi
          
          if [ -f k8s/monitoring/alerts.yaml ]; then
            kubectl apply -f k8s/monitoring/alerts.yaml
            echo "‚úÖ PrometheusRules applied"
          fi
          
          echo "‚úÖ All monitoring manifests applied"
      
      - name: Verify Monitoring Deployment
        run: |
          echo "=== üìä Monitoring Services ==="
          kubectl get svc -n monitoring
          echo ""
          echo "=== üìä Monitoring Pods ==="
          kubectl get pods -n monitoring
          echo ""
          echo "=== üìä ServiceMonitors ==="
          kubectl get servicemonitor -n monitoring
          echo ""
          echo "=== üìä PrometheusRules ==="
          kubectl get prometheusrule -n monitoring
          echo ""
          echo "‚úÖ Monitoring verification completed"
