name: infra-complete

on:
  workflow_dispatch:
  push:
    branches: [ main ]

permissions:
  id-token: write
  contents: read

env:
  TF_IN_AUTOMATION: true
  ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
  ARM_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
  ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
  ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

jobs:
  terraform:
    runs-on: ubuntu-latest
    outputs:
      kubeconfig: ${{ steps.capture.outputs.kubeconfig }}
      key_vault_name: ${{ steps.capture.outputs.key_vault_name }}
      aks_uami_client_id: ${{ steps.capture.outputs.aks_uami_client_id }}
      tenant_id: ${{ steps.capture.outputs.tenant_id }}
      ingress_static_ip: ${{ steps.capture.outputs.ingress_static_ip }}
      aks_node_rg: ${{ steps.capture.outputs.aks_node_rg }}
      frontdoor_endpoint: ${{ steps.capture.outputs.frontdoor_endpoint }}
      resource_group_name: ${{ steps.capture.outputs.resource_group_name }}
    steps:
      - uses: actions/checkout@v4

      - name: Azure login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - uses: hashicorp/setup-terraform@v3

      - name: Terraform init
        working-directory: terraform
        run: terraform init -input=false

      - name: Terraform apply
        working-directory: terraform
        env:
          TF_VAR_sql_password: ${{ secrets.SQL_ADMIN_PASSWORD }}
        run: terraform apply -auto-approve

      - name: Capture Terraform outputs
        id: capture
        working-directory: terraform
        run: |
          KUBECONFIG_B64=$(terraform output -raw aks_kube_config | base64 | tr -d '\n')
          echo "kubeconfig=$KUBECONFIG_B64" >> $GITHUB_OUTPUT
          echo "key_vault_name=$(terraform output -raw key_vault_name)" >> $GITHUB_OUTPUT
          echo "aks_uami_client_id=$(terraform output -raw aks_user_assigned_identity_client_id)" >> $GITHUB_OUTPUT
          echo "tenant_id=$(terraform output -raw tenant_id)" >> $GITHUB_OUTPUT
          echo "ingress_static_ip=$(terraform output -raw ingress_static_ip)" >> $GITHUB_OUTPUT
          if terraform output -raw frontdoor_endpoint >/dev/null 2>&1; then
            echo "frontdoor_endpoint=$(terraform output -raw frontdoor_endpoint)" >> $GITHUB_OUTPUT
          fi
          AKS_NODE_RG=$(terraform output -raw aks_node_resource_group)
          echo "aks_node_rg=$AKS_NODE_RG" >> $GITHUB_OUTPUT
          # Get resource group name from Terraform state
          RG_NAME=$(terraform show -json | jq -r '.values.root_module.resources[] | select(.type == "azurerm_resource_group") | .values.name' | head -1 || echo "rg-group1-austriaeast")
          echo "resource_group_name=$RG_NAME" >> $GITHUB_OUTPUT


  deploy:
    runs-on: ubuntu-latest
    needs: terraform
    environment: production
    env:
      INGRESS_STATIC_IP: ${{ needs.terraform.outputs.ingress_static_ip }}
      AKS_NODE_RG: ${{ needs.terraform.outputs.aks_node_rg }}
      FRONTDOOR_ENDPOINT: ${{ needs.terraform.outputs.frontdoor_endpoint }}
      RESOURCE_GROUP: ${{ needs.terraform.outputs.resource_group_name }}
    steps:
      - uses: actions/checkout@v4

      - name: Azure login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Configure kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "${{ needs.terraform.outputs.kubeconfig }}" | base64 -d > ~/.kube/config
          chmod 600 ~/.kube/config
          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Test kubectl connectivity
        run: kubectl get nodes
      
      - name: Install Helm
        uses: azure/setup-helm@v4


      # âœ… Use Static IP from Terraform
      - name: Get Static IP from Terraform
        run: |
          echo "âœ… Using Static IP from Terraform: $INGRESS_STATIC_IP"
          echo "âœ… AKS Node Resource Group: $AKS_NODE_RG"

      # âœ… Ingress Install/Upgrade
      - name: Install/Upgrade Ingress NGINX (with static IP)
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm upgrade --install grp1-ingress ingress-nginx/ingress-nginx \
              --namespace ingress-nginx \
              --create-namespace \
            --atomic \
              --wait \
              --timeout=10m \
            --set controller.service.loadBalancerIP="$INGRESS_STATIC_IP" \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/azure-load-balancer-resource-group"="$AKS_NODE_RG"

      - name: Auto-update Front Door when Ingress IP changes
        if: env.FRONTDOOR_ENDPOINT != ''
        run: |
          echo "ðŸ” Waiting for Ingress Service IP assignment..."
          ACTUAL_IP=""
          for i in {1..30}; do
            ACTUAL_IP=$(kubectl get svc grp1-ingress-ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
            if [ -n "$ACTUAL_IP" ]; then
              echo "âœ… Ingress Service IP: $ACTUAL_IP"
              break
            fi
            echo "â³ Waiting... ($i/30)"
            sleep 5
          done
          
          if [ -z "$ACTUAL_IP" ]; then
            echo "âš ï¸  IP not assigned, using Static IP: $INGRESS_STATIC_IP"
            ACTUAL_IP="$INGRESS_STATIC_IP"
          fi
          
          echo "ðŸ”„ Updating Front Door origin to use IP: $ACTUAL_IP"
          
          # Update Front Door origin via Azure CLI
          echo "ðŸ“¦ Installing Azure Front Door extension..."
          az extension add --name front-door --yes 2>/dev/null || az extension update --name front-door 2>/dev/null || true
          
          # Use resource group from Terraform outputs
          RG_NAME="${RESOURCE_GROUP:-rg-group1-austriaeast}"
          echo "ðŸ“‹ Using Resource Group: $RG_NAME"
          
          echo "ðŸ”„ Updating Front Door origin..."
          az cdn afd origin update \
            --resource-group "$RG_NAME" \
            --profile-name grp1-afd-profile \
            --origin-group-name grp1-origin-group \
            --origin-name grp1-origin \
            --host-name "$ACTUAL_IP" \
            --origin-host-header "$ACTUAL_IP.nip.io" \
            --http-port 80 \
            --https-port 443 \
            --enabled true \
            --no-wait || {
            echo "âš ï¸  Failed to update Front Door origin"
            echo "   This might be due to the origin already having these values"
            echo "   Front Door will continue using the current configuration"
          }
          
          echo "âœ… Front Door origin update initiated!"
          echo "   Host: $ACTUAL_IP"
          echo "   Origin Host Header: $ACTUAL_IP.nip.io"
          echo "   (Changes may take 2-5 minutes to propagate)"

      - name: DNS instructions (CNAME to Front Door)
        if: env.FRONTDOOR_ENDPOINT != ''
        run: |
          echo "\n================ DNS Instructions ================\n"
          echo "Recommended: point your domain CNAME to:"
          echo "  $FRONTDOOR_ENDPOINT"
          echo "This keeps your domain stable even if the ingress IP changes.\n"
          echo "Example (Cloudflare):"
          echo "  Type: CNAME | Name: www | Target: $FRONTDOOR_ENDPOINT"
          echo "\nQuick verify (Front Door):"
          curl -I --max-time 10 "https://$FRONTDOOR_ENDPOINT" || true

      - name: Optional - Update DNS via Cloudflare (A or CNAME)
        if: env.DNS_PROVIDER == 'cloudflare'
        env:
          CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
          CF_ZONE_ID: ${{ secrets.CF_ZONE_ID }}
          DNS_RECORD_NAME: ${{ secrets.DNS_RECORD_NAME }}
          DNS_RECORD_TYPE: ${{ secrets.DNS_RECORD_TYPE }}
        run: |
          if [ -z "$CF_API_TOKEN" ] || [ -z "$CF_ZONE_ID" ] || [ -z "$DNS_RECORD_NAME" ]; then
            echo "Cloudflare variables are not set. Skipping DNS update."; exit 0; fi
          # Determine record type and target
          if [ -n "$DNS_RECORD_TYPE" ]; then TYPE="$DNS_RECORD_TYPE"; else
            if [ -n "$FRONTDOOR_ENDPOINT" ]; then TYPE=CNAME; else TYPE=A; fi
          fi
          if [ "$TYPE" = "CNAME" ]; then TARGET="$FRONTDOOR_ENDPOINT"; else TARGET="$INGRESS_STATIC_IP"; fi
          echo "Updating Cloudflare DNS: $DNS_RECORD_NAME ($TYPE) -> $TARGET"
          REC_ID=$(curl -s -X GET "https://api.cloudflare.com/client/v4/zones/$CF_ZONE_ID/dns_records?name=$DNS_RECORD_NAME" \
            -H "Authorization: Bearer $CF_API_TOKEN" -H "Content-Type: application/json" | jq -r '.result[0].id // empty')
          if [ -n "$REC_ID" ]; then METHOD=PUT; URL="https://api.cloudflare.com/client/v4/zones/$CF_ZONE_ID/dns_records/$REC_ID"; else METHOD=POST; URL="https://api.cloudflare.com/client/v4/zones/$CF_ZONE_ID/dns_records"; fi
          DATA=$(jq -n --arg type "$TYPE" --arg name "$DNS_RECORD_NAME" --arg content "$TARGET" '{type:$type,name:$name,content:$content,ttl:120,proxied:true}')
          curl -s -X $METHOD "$URL" -H "Authorization: Bearer $CF_API_TOKEN" -H "Content-Type: application/json" --data "$DATA" | jq -r '.success'


      # âœ… Namespaces and Config
      - name: Apply namespaces and config maps
        run: |
          kubectl apply -f k8s/namespace.yaml
          kubectl apply -f k8s/backend/configmap.yaml
          kubectl apply -f k8s/frontend/configmap.yaml


      # âœ… Key Vault CSI Driver + Secret Sync
      - name: Apply SecretProviderClass and Synced secret
        env:
          KEY_VAULT_NAME: ${{ needs.terraform.outputs.key_vault_name }}
          AKS_UAMI_CLIENT_ID: ${{ needs.terraform.outputs.aks_uami_client_id }}
          AZURE_TENANT_ID: ${{ needs.terraform.outputs.tenant_id }}
        run: |
          envsubst < k8s/backend/secretproviderclass.yaml | kubectl apply -f -
          kubectl apply -f k8s/backend/app-secret.yaml


      # âœ… Backend Deploy (Ù…Ø¹ Ø§Ù†ØªØ¸Ø§Ø± Ø£Ø·ÙˆÙ„ + ØµØ­ÙŠ)
      - name: Deploy backend workloads
        run: |
          kubectl apply -f k8s/backend/service.yaml
          kubectl apply -f k8s/backend/deployment.yaml
          kubectl apply -f k8s/backend/backendHPA.yaml
          
          # ðŸ” Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙƒÙ„ Pods ØªÙƒÙˆÙ† Ø¬Ø§Ù‡Ø²Ø©
          kubectl rollout status deploy/app-backend-deployment -n app --timeout=600s
          kubectl wait pods -n app -l app=backend --for=condition=Ready --timeout=300s


      # âœ… Frontend Deploy
      - name: Deploy frontend workloads
        run: |
          kubectl apply -f k8s/frontend/service.yaml
          kubectl apply -f k8s/frontend/deployment.yaml
          kubectl apply -f k8s/frontend/frontendHPA.yaml
          kubectl rollout status deploy/app-frontend-deployment -n app --timeout=300s


      # âœ… Monitoring with Helm (âœ… Fixed)
      - name: Deploy/Upgrade Monitoring Stack
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              --namespace monitoring \
              --create-namespace \
              --values k8s/monitoring/values.yaml \
            --atomic \
              --wait \
            --timeout=15m

      
      # âœ… Ingress Rules
      - name: Apply ingress rules
        run: |
          kubectl apply -f k8s/backend/ingress.yaml
          kubectl apply -f k8s/frontend/ingress.yaml


      # âœ… Final cluster health check
      - name: Validate cluster health
        run: |
          kubectl get pods -A
          kubectl get ing -A
          kubectl get svc -A
